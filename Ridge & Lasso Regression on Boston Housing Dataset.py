# -*- coding: utf-8 -*-
"""HW_REGRESSION_Dimas_Adi_Prasetyo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uL3unyXtzN7yxAHkR4iHaTDag6ZT7ttP

# HW_REGRESSION_Dimas Adi Prasetyo

Assignment: Ridge & Lasso Regression on Boston Housing Dataset

## 1. Import Libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

"""## 2. Load Dataset"""

from google.colab import drive
drive.mount('/content/drive')

boston = pd.read_csv('/content/drive/MyDrive/Dibimbing Bootcamp Data Science/boston.csv')
boston.head(5)

missing_values = boston.isnull().sum()
missing_values

duplicated = boston.duplicated().sum()
duplicated

"""## 3. Pisahkan Fitur dan Target"""

feature = boston.drop(columns='medv')
target = boston[['medv']]

"""## 4. Split Data (Train, Validation, Test)"""

# First splitting: pretrain and test
feature_boston_pretrain, feature_boston_test, target_boston_pretrain, target_boston_test = train_test_split(feature, target, test_size=0.20, random_state=42)

# Second splitting: train and validation
feature_boston_train, feature_boston_validation, target_boston_train, target_boston_validation = train_test_split(feature_boston_pretrain, target_boston_pretrain, test_size=0.20, random_state=42)

feature_boston_train.shape, feature_boston_test.shape

"""## 5. Correlation Matrix"""

# calculate VIF scores
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

X = add_constant(feature_boston_train)

vif_df = pd.DataFrame([vif(X.values, i)
               for i in range(X.shape[1])],
              index=X.columns).reset_index()
vif_df.columns = ['feature','vif_score']
vif_df = vif_df.loc[vif_df.feature!='const']
vif_df

# heatmap correlation
data_train = pd.concat([feature_boston_train, target_boston_train], axis=1)
corr = data_train.corr()

plt.figure(figsize=(12, 9))
sns.heatmap(corr, annot=True, fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

"""### Pemilihan Fitur Berdasarkan Korelasi

Untuk mengurangi multikolinearitas, fitur dengan koefisien korelasi di atas **0,8** ditinjau.
Misalnya, `RAD` dan `TAX` menunjukkan korelasi yang kuat. Menyertakan keduanya dapat menyebabkan redundansi, jadi saya mempertimbangkan untuk menghilangkan satu untuk meningkatkan stabilitas model.

## 6. Drop Fitur dengan Multikolinearitas Tinggi
"""

feature_boston_train = feature_boston_train.drop(columns=['rad'])
feature_boston_validation = feature_boston_validation.drop(columns=['rad'])
feature_boston_test = feature_boston_test.drop(columns=['rad'])

"""Alasan mendrop 'rad':

*  VIF tinggi.
*  Korelasi sangat tinggi dengan tax yang mana berarti informasi serupa/mirip.
*  lebih baik simpan tax karena lebih interpretatif (jumlah pajak properti)
*  Menurunkan risiko overfitting karena multikolinearitas.
"""

# calculate VIF scores
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

X = add_constant(feature_boston_train)

vif_df = pd.DataFrame([vif(X.values, i)
               for i in range(X.shape[1])],
              index=X.columns).reset_index()
vif_df.columns = ['feature','vif_score']
vif_df = vif_df.loc[vif_df.feature!='const']
vif_df

"""## 7. Ridge Regression dengan Validasi MAE, MAPE, dan RMSE"""

# train the model
X_boston_train = feature_boston_train.to_numpy()
y_boston_train = target_boston_train.to_numpy()
y_boston_train = y_boston_train.reshape(len(y_boston_train),)

# define the model
# 4 models with 4 different alphas (lambda)
ridge_reg_pointzeroone = Ridge(alpha=0.01, random_state=42)
ridge_reg_pointone = Ridge(alpha=0.1, random_state=42)
ridge_reg_one = Ridge(alpha=1, random_state=42)
ridge_reg_ten = Ridge(alpha=10, random_state=42)

# fit the model (training)
ridge_reg_pointzeroone.fit(X_boston_train, y_boston_train)
ridge_reg_pointone.fit(X_boston_train, y_boston_train)
ridge_reg_one.fit(X_boston_train, y_boston_train)
ridge_reg_ten.fit(X_boston_train, y_boston_train)

from sklearn.metrics import mean_squared_error

X_boston_validation = feature_boston_validation.to_numpy()
y_boston_validation = target_boston_validation.to_numpy()
y_boston_validation = y_boston_validation.reshape(len(y_boston_validation),)

alphas = [0.01, 0.1, 1., 10]
models = [ridge_reg_pointzeroone,
          ridge_reg_pointone,
          ridge_reg_one,
          ridge_reg_ten]

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation,y_predict_validation))
    print(f'RMSE of Ridge regression model with alpha = {alpha} is {rmse}')

from sklearn.metrics import mean_absolute_percentage_error

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mape = mean_absolute_percentage_error(y_boston_validation, y_predict_validation)
    print(f'MAPE of Ridge regression model with alpha = {alpha} is {mape:.4f}')

from sklearn.metrics import mean_absolute_error
for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mae = mean_absolute_error(y_boston_validation,y_predict_validation)
    print(f'MAE of Ridge regression model with alpha = {alpha} is {mae}')

"""## 8. Lasso Regression dengan Validasi MAE, MAPE, dan RMSE"""

from sklearn.linear_model import Lasso

# train the model
X_boston_train = feature_boston_train.to_numpy()
y_boston_train = target_boston_train.to_numpy()
y_boston_train = y_boston_train.reshape(len(y_boston_train),)

# define the model
# 4 models with 4 different alphas (lambda)
lasso_reg_pointzeroone = Lasso(alpha=0.01, random_state=42)
lasso_reg_pointone = Lasso(alpha=0.1, random_state=42)
lasso_reg_one = Lasso(alpha=1, random_state=42)
lasso_reg_ten = Lasso(alpha=10, random_state=42)

# fit the model (training)
lasso_reg_pointzeroone.fit(X_boston_train, y_boston_train)
lasso_reg_pointone.fit(X_boston_train, y_boston_train)
lasso_reg_one.fit(X_boston_train, y_boston_train)
lasso_reg_ten.fit(X_boston_train, y_boston_train)

from sklearn.metrics import mean_squared_error

X_boston_validation = feature_boston_validation.to_numpy()
y_boston_validation = target_boston_validation.to_numpy()
y_boston_validation = y_boston_validation.reshape(len(y_boston_validation),)

alphas = [0.01, 0.1, 1., 10]
models = [lasso_reg_pointzeroone,
          lasso_reg_pointone,
          lasso_reg_one,
          lasso_reg_ten]

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation, y_predict_validation))
    print(f'RMSE of Lasso regression model with alpha = {alpha} is {rmse:.4f}')

from sklearn.metrics import mean_absolute_percentage_error

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mape = mean_absolute_percentage_error(y_boston_validation, y_predict_validation)
    print(f'MAPE of Lasso regression model with alpha = {alpha} is {mape:.4f}')

from sklearn.metrics import mean_absolute_error

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mae = mean_absolute_error(y_boston_validation, y_predict_validation)
    print(f'MAE of Lasso regression model with alpha = {alpha} is {mae:.4f}')

"""## 9. Train Model Final dan Interpretasi Koefisien"""

# Simpan hasil RMSE Ridge
ridge_results = {}

for model, alpha in zip(models, alphas):
    y_pred = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation, y_pred))
    ridge_results[alpha] = rmse

# Lihat hasilnya
print("Ridge RMSE per alpha:", ridge_results)

# Simpan hasil RMSE Lasso
lasso_results = {}

for model, alpha in zip(models, alphas):
    y_pred = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation, y_pred))
    lasso_results[alpha] = rmse

# Lihat hasilnya
print("Lasso RMSE per alpha:", lasso_results)

best_lambda_ridge = min(ridge_results, key=ridge_results.get)
best_lambda_lasso = min(lasso_results, key=lasso_results.get)

# Gabungkan training dan validation
X_combined = pd.concat([feature_boston_train, feature_boston_validation])
y_combined = pd.concat([target_boston_train, target_boston_validation])

# Ambil alpha terbaik
best_lambda_ridge = min(ridge_results, key=ridge_results.get)
best_lambda_lasso = min(lasso_results, key=lasso_results.get)

# Train final model
final_ridge = Ridge(alpha=best_lambda_ridge, random_state=42).fit(X_combined, y_combined)
final_lasso = Lasso(alpha=best_lambda_lasso, random_state=42).fit(X_combined, y_combined)

# Gabungkan training dan validation
X_combined = pd.concat([feature_boston_train, feature_boston_validation])
y_combined = pd.concat([target_boston_train, target_boston_validation])

# Ambil alpha terbaik
best_lambda_ridge = min(ridge_results, key=ridge_results.get)
best_lambda_lasso = min(lasso_results, key=lasso_results.get)

# Train final model
final_ridge = Ridge(alpha=best_lambda_ridge, random_state=42).fit(X_combined, y_combined)
final_lasso = Lasso(alpha=best_lambda_lasso, random_state=42).fit(X_combined, y_combined)

# Tampilkan koefisien
ridge_coefficients = pd.Series(final_ridge.coef_, index=feature_boston_train.columns)
lasso_coefficients = pd.Series(final_lasso.coef_, index=feature_boston_train.columns)

# Gabungkan dalam satu DataFrame untuk dibandingkan
coeff_comparison = pd.DataFrame({
    'Ridge Coefficients': ridge_coefficients,
    'Lasso Coefficients': lasso_coefficients
})

coeff_comparison.sort_values('Ridge Coefficients', ascending=False)

# Calculate predictions on the training set using the best model (final_lasso)
y_predict_train = final_lasso.predict(X_combined)

# calculate residuals
residual = y_combined.values.flatten() - y_predict_train # Ensure y_combined is a numpy array and flattened for subtraction

# prepare dataframe
# predictor (predicted value) VS residual
df_resid = pd.DataFrame({
    'predicted_value': y_predict_train,
    'residual': residual
})

# residual plot
sns.scatterplot(data=df_resid, x="predicted_value", y="residual")
plt.axhline(0)
plt.title('Residual Plot on Combined Training and Validation Set')
plt.xlabel('Predicted Value')
plt.ylabel('Residual')
plt.show()

sns.distplot(residual)

# QQplot
from sklearn.preprocessing import StandardScaler

std_resid = StandardScaler().fit_transform(residual.reshape(-1,1))
std_resid = np.array([value for nested_array in std_resid for value in nested_array])

import statsmodels.api as sm
sm.qqplot(std_resid, line='45')
plt.show()

ridge_coef = pd.Series(final_ridge.coef_, index=X_combined.columns)
lasso_coef = pd.Series(final_lasso.coef_, index=X_combined.columns)

print("Ridge Coefficients:\n", ridge_coef)
print("Lasso Coefficients:\n", lasso_coef)

def evaluate(model, X, y):
    y_pred = model.predict(X)
    mae = mean_absolute_error(y, y_pred)
    mape = mean_absolute_percentage_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    return mae, mape, rmse

print(f"\nBest Ridge alpha: {best_lambda_ridge}")
print("Ridge - MAE, MAPE, RMSE:", evaluate(final_ridge, feature_boston_test, target_boston_test))

print(f"\nBest Lasso alpha: {best_lambda_lasso}")
print("Lasso - MAE, MAPE, RMSE:", evaluate(final_lasso, feature_boston_test, target_boston_test))

"""## Interpretasi Hasil
* Secara umum, performa kedua model cukup sebanding, dengan nilai RMSE yang hampir sama (selisih hanya 0.00002).

* Ridge Regression menunjukkan MAE dan MAPE yang lebih rendah sedikit, menandakan prediksi yang lebih stabil dan konsisten dalam meminimalkan kesalahan rata-rata.

* Lasso Regression sedikit lebih buruk dalam MAE dan MAPE, namun memiliki keunggulan dalam seleksi fitur otomatisâ€”beberapa fitur seperti chas dan nox memiliki koefisien nol, artinya model menganggap fitur tersebut tidak signifikan.

## Kesimpulan
Model terbaik secara keseluruhan adalah Lasso Regression dengan alpha = 1.0, karena:

*  RMSE paling rendah (5.1421)

*  MAE masih kompetitif (3.4497)

*  MAPE masih dalam batas wajar (17.88%)
"""