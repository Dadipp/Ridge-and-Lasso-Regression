# -*- coding: utf-8 -*-
"""HW_REGRESSION_Dimas_Adi_Prasetyo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uL3unyXtzN7yxAHkR4iHaTDag6ZT7ttP

# HW_REGRESSION_Dimas Adi Prasetyo

Assignment: Ridge & Lasso Regression on Boston Housing Dataset

## 1. Import Libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

"""## 2. Load Dataset"""

from google.colab import drive
drive.mount('/content/drive')

boston = pd.read_csv('/content/drive/MyDrive/Dibimbing Bootcamp Data Science/boston.csv')
boston.head(5)

missing_values = boston.isnull().sum()
missing_values

duplicated = boston.duplicated().sum()
duplicated

"""## 3. Pisahkan Fitur dan Target"""

feature = boston.drop(columns='medv')
target = boston[['medv']]

"""## 4. Split Data (Train, Validation, Test)"""

# First splitting: pretrain and test
feature_boston_pretrain, feature_boston_test, target_boston_pretrain, target_boston_test = train_test_split(feature, target, test_size=0.20, random_state=42)

# Second splitting: train and validation
feature_boston_train, feature_boston_validation, target_boston_train, target_boston_validation = train_test_split(feature_boston_pretrain, target_boston_pretrain, test_size=0.20, random_state=42)

feature_boston_train.shape, feature_boston_test.shape

"""## 5. Correlation Matrix"""

# calculate VIF scores
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

X = add_constant(feature_boston_train)

vif_df = pd.DataFrame([vif(X.values, i)
               for i in range(X.shape[1])],
              index=X.columns).reset_index()
vif_df.columns = ['feature','vif_score']
vif_df = vif_df.loc[vif_df.feature!='const']
vif_df

# heatmap correlation
data_train = pd.concat([feature_boston_train, target_boston_train], axis=1)
corr = data_train.corr()

plt.figure(figsize=(12, 9))
sns.heatmap(corr, annot=True, fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

"""### Pemilihan Fitur Berdasarkan Korelasi

Untuk mengurangi multikolinearitas, fitur dengan koefisien korelasi di atas **0,8** ditinjau.
Misalnya, `RAD` dan `TAX` menunjukkan korelasi yang kuat. Menyertakan keduanya dapat menyebabkan redundansi, jadi saya mempertimbangkan untuk menghilangkan satu untuk meningkatkan stabilitas model.

## 6. Drop Fitur dengan Multikolinearitas Tinggi
"""

feature_boston_train = feature_boston_train.drop(columns=['rad'])
feature_boston_validation = feature_boston_validation.drop(columns=['rad'])
feature_boston_test = feature_boston_test.drop(columns=['rad'])

"""Alasan mendrop 'rad':

*  VIF tinggi.
*  Korelasi sangat tinggi dengan tax yang mana berarti informasi serupa/mirip.
*  lebih baik simpan tax karena lebih interpretatif (jumlah pajak properti)
*  Menurunkan risiko overfitting karena multikolinearitas.
"""

# calculate VIF scores
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

X = add_constant(feature_boston_train)

vif_df = pd.DataFrame([vif(X.values, i)
               for i in range(X.shape[1])],
              index=X.columns).reset_index()
vif_df.columns = ['feature','vif_score']
vif_df = vif_df.loc[vif_df.feature!='const']
vif_df

"""## 7. Ridge Regression dengan Validasi MAE, MAPE, dan RMSE"""

# train the model
X_boston_train = feature_boston_train.to_numpy()
y_boston_train = target_boston_train.to_numpy()
y_boston_train = y_boston_train.reshape(len(y_boston_train),)

# define the model
# 4 models with 4 different alphas (lambda)
ridge_reg_pointzeroone = Ridge(alpha=0.01, random_state=42)
ridge_reg_pointone = Ridge(alpha=0.1, random_state=42)
ridge_reg_one = Ridge(alpha=1, random_state=42)
ridge_reg_ten = Ridge(alpha=10, random_state=42)

# fit the model (training)
ridge_reg_pointzeroone.fit(X_boston_train, y_boston_train)
ridge_reg_pointone.fit(X_boston_train, y_boston_train)
ridge_reg_one.fit(X_boston_train, y_boston_train)
ridge_reg_ten.fit(X_boston_train, y_boston_train)

from sklearn.metrics import mean_squared_error

X_boston_validation = feature_boston_validation.to_numpy()
y_boston_validation = target_boston_validation.to_numpy()
y_boston_validation = y_boston_validation.reshape(len(y_boston_validation),)

alphas = [0.01, 0.1, 1., 10]
models = [ridge_reg_pointzeroone,
          ridge_reg_pointone,
          ridge_reg_one,
          ridge_reg_ten]

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation,y_predict_validation))
    print(f'RMSE of Ridge regression model with alpha = {alpha} is {rmse}')

from sklearn.metrics import mean_absolute_percentage_error

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mape = mean_absolute_percentage_error(y_boston_validation, y_predict_validation)
    print(f'MAPE of Ridge regression model with alpha = {alpha} is {mape:.4f}')

from sklearn.metrics import mean_absolute_error
for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mae = mean_absolute_error(y_boston_validation,y_predict_validation)
    print(f'MAE of Ridge regression model with alpha = {alpha} is {mae}')

"""## 8. Lasso Regression dengan Validasi MAE, MAPE, dan RMSE"""

from sklearn.linear_model import Lasso

# train the model
X_boston_train = feature_boston_train.to_numpy()
y_boston_train = target_boston_train.to_numpy()
y_boston_train = y_boston_train.reshape(len(y_boston_train),)

# define the model
# 4 models with 4 different alphas (lambda)
lasso_reg_pointzeroone = Lasso(alpha=0.01, random_state=42)
lasso_reg_pointone = Lasso(alpha=0.1, random_state=42)
lasso_reg_one = Lasso(alpha=1, random_state=42)
lasso_reg_ten = Lasso(alpha=10, random_state=42)

# fit the model (training)
lasso_reg_pointzeroone.fit(X_boston_train, y_boston_train)
lasso_reg_pointone.fit(X_boston_train, y_boston_train)
lasso_reg_one.fit(X_boston_train, y_boston_train)
lasso_reg_ten.fit(X_boston_train, y_boston_train)

from sklearn.metrics import mean_squared_error

X_boston_validation = feature_boston_validation.to_numpy()
y_boston_validation = target_boston_validation.to_numpy()
y_boston_validation = y_boston_validation.reshape(len(y_boston_validation),)

alphas = [0.01, 0.1, 1., 10]
models = [lasso_reg_pointzeroone,
          lasso_reg_pointone,
          lasso_reg_one,
          lasso_reg_ten]

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation, y_predict_validation))
    print(f'RMSE of Lasso regression model with alpha = {alpha} is {rmse:.4f}')

from sklearn.metrics import mean_absolute_percentage_error

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mape = mean_absolute_percentage_error(y_boston_validation, y_predict_validation)
    print(f'MAPE of Lasso regression model with alpha = {alpha} is {mape:.4f}')

from sklearn.metrics import mean_absolute_error

for model, alpha in zip(models, alphas):
    y_predict_validation = model.predict(X_boston_validation)
    mae = mean_absolute_error(y_boston_validation, y_predict_validation)
    print(f'MAE of Lasso regression model with alpha = {alpha} is {mae:.4f}')

"""## 9. Train Model Final dan Interpretasi Koefisien"""

# Simpan hasil RMSE Ridge
ridge_results = {}

for model, alpha in zip(models, alphas):
    y_pred = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation, y_pred))
    ridge_results[alpha] = rmse

# Lihat hasilnya
print("Ridge RMSE per alpha:", ridge_results)

# Simpan hasil RMSE Lasso
lasso_results = {}

for model, alpha in zip(models, alphas):
    y_pred = model.predict(X_boston_validation)
    rmse = np.sqrt(mean_squared_error(y_boston_validation, y_pred))
    lasso_results[alpha] = rmse

# Lihat hasilnya
print("Lasso RMSE per alpha:", lasso_results)

best_lambda_ridge = min(ridge_results, key=ridge_results.get)
best_lambda_lasso = min(lasso_results, key=lasso_results.get)

# Gabungkan training dan validation
X_combined = pd.concat([feature_boston_train, feature_boston_validation])
y_combined = pd.concat([target_boston_train, target_boston_validation])

# Ambil alpha terbaik
best_lambda_ridge = min(ridge_results, key=ridge_results.get)
best_lambda_lasso = min(lasso_results, key=lasso_results.get)

# Train final model
final_ridge = Ridge(alpha=best_lambda_ridge, random_state=42).fit(X_combined, y_combined)
final_lasso = Lasso(alpha=best_lambda_lasso, random_state=42).fit(X_combined, y_combined)

# Gabungkan training dan validation
X_combined = pd.concat([feature_boston_train, feature_boston_validation])
y_combined = pd.concat([target_boston_train, target_boston_validation])

# Ambil alpha terbaik
best_lambda_ridge = min(ridge_results, key=ridge_results.get)
best_lambda_lasso = min(lasso_results, key=lasso_results.get)

# Train final model
final_ridge = Ridge(alpha=best_lambda_ridge, random_state=42).fit(X_combined, y_combined)
final_lasso = Lasso(alpha=best_lambda_lasso, random_state=42).fit(X_combined, y_combined)

# Tampilkan koefisien
ridge_coefficients = pd.Series(final_ridge.coef_, index=feature_boston_train.columns)
lasso_coefficients = pd.Series(final_lasso.coef_, index=feature_boston_train.columns)

# Gabungkan dalam satu DataFrame untuk dibandingkan
coeff_comparison = pd.DataFrame({
    'Ridge Coefficients': ridge_coefficients,
    'Lasso Coefficients': lasso_coefficients
})

coeff_comparison.sort_values('Ridge Coefficients', ascending=False)

# Calculate predictions on the training set using the best model (final_lasso)
y_predict_train = final_lasso.predict(X_combined)

# calculate residuals
residual = y_combined.values.flatten() - y_predict_train # Ensure y_combined is a numpy array and flattened for subtraction

# prepare dataframe
# predictor (predicted value) VS residual
df_resid = pd.DataFrame({
    'predicted_value': y_predict_train,
    'residual': residual
})

# residual plot
sns.scatterplot(data=df_resid, x="predicted_value", y="residual")
plt.axhline(0)
plt.title('Residual Plot on Combined Training and Validation Set')
plt.xlabel('Predicted Value')
plt.ylabel('Residual')
plt.show()

sns.distplot(residual)

# QQplot
from sklearn.preprocessing import StandardScaler

std_resid = StandardScaler().fit_transform(residual.reshape(-1,1))
std_resid = np.array([value for nested_array in std_resid for value in nested_array])

import statsmodels.api as sm
sm.qqplot(std_resid, line='45')
plt.show()

ridge_coef = pd.Series(final_ridge.coef_, index=X_combined.columns)
lasso_coef = pd.Series(final_lasso.coef_, index=X_combined.columns)

print("Ridge Coefficients:\n", ridge_coef)
print("Lasso Coefficients:\n", lasso_coef)

def evaluate(model, X, y):
    y_pred = model.predict(X)
    mae = mean_absolute_error(y, y_pred)
    mape = mean_absolute_percentage_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    return mae, mape, rmse

print(f"\nBest Ridge alpha: {best_lambda_ridge}")
print("Ridge - MAE, MAPE, RMSE:", evaluate(final_ridge, feature_boston_test, target_boston_test))

print(f"\nBest Lasso alpha: {best_lambda_lasso}")
print("Lasso - MAE, MAPE, RMSE:", evaluate(final_lasso, feature_boston_test, target_boston_test))

"""## Interpretasi Hasil
* Secara umum, performa kedua model cukup sebanding, dengan nilai RMSE yang hampir sama (selisih hanya 0.00002).

* Ridge Regression menunjukkan MAE dan MAPE yang lebih rendah sedikit, menandakan prediksi yang lebih stabil dan konsisten dalam meminimalkan kesalahan rata-rata.

* Lasso Regression sedikit lebih buruk dalam MAE dan MAPE, namun memiliki keunggulan dalam seleksi fitur otomatis—beberapa fitur seperti chas dan nox memiliki koefisien nol, artinya model menganggap fitur tersebut tidak signifikan.

## Kesimpulan
Model terbaik secara keseluruhan adalah Lasso Regression dengan alpha = 1.0, karena:

*  RMSE paling rendah (5.1421)

*  MAE masih kompetitif (3.4497)

*  MAPE masih dalam batas wajar (17.88%)
"""